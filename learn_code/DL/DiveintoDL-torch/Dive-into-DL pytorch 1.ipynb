{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[0.0000e+00, 0.0000e+00, 7.7052e+31],\n",
      "        [7.2148e+22, 2.5226e-18, 6.4825e-10],\n",
      "        [1.0072e-11, 3.0880e-09, 1.0504e-05],\n",
      "        [4.1656e-11, 1.0503e-05, 2.9571e-18],\n",
      "        [6.7333e+22, 1.7591e+22, 1.7184e+25]])\n",
      "new_x: tensor([[-1.0000e+00, -1.0000e+00,  7.7052e+31],\n",
      "        [ 7.2148e+22, -1.0000e+00, -1.0000e+00],\n",
      "        [-1.0000e+00, -1.0000e+00, -9.9999e-01],\n",
      "        [-1.0000e+00, -9.9999e-01, -1.0000e+00],\n",
      "        [ 6.7333e+22,  1.7591e+22,  1.7184e+25]])\n",
      "x_cp: tensor([0.0000e+00, 0.0000e+00, 7.7052e+31, 7.2148e+22, 2.5226e-18, 6.4825e-10,\n",
      "        1.0072e-11, 3.0880e-09, 1.0504e-05, 4.1656e-11, 1.0503e-05, 2.9571e-18,\n",
      "        6.7333e+22, 1.7591e+22, 1.7184e+25])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import numpy as np\n",
    "x = torch.empty(5,3)\n",
    "b = torch.eye(5,3)\n",
    "c = torch.randperm(10)\n",
    "# print(b.add_(x))\n",
    "# print(x.shape)\n",
    "# print(b)\n",
    "# print(c)\n",
    "# y = x.view(15)\n",
    "# z = x.view(-1, 5)  # -1所指的维度可以根据其他维度的值推出来\n",
    "# print(x,x.size(),y, y.size(),z, z.size())\n",
    "\n",
    "print('x:',x)\n",
    "x_cp = x.clone().view(15)  #使用clone还有一个好处是会被记录在计算图中，即梯度回传到副本时也会传到源Tensor。\n",
    "x -= 1 \n",
    "print('new_x:',x)\n",
    "print(\"x_cp:\",x_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6923])\n",
      "-0.6923133730888367\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())  #.item():, 它可以将一个标量Tensor转换成一个Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-22-684d901cf36e>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-22-684d901cf36e>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    “““\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "“““\n",
    "函数\t                                功能\n",
    "trace\t                           对角线元素之和(矩阵的迹)\n",
    "diag\t                           对角线元素\n",
    "triu/tril\t                       矩阵的上三角/下三角，可指定偏移量\n",
    "mm/bmm\t                           矩阵乘法，batch的矩阵乘法\n",
    "addmm/addbmm/addmv/addr/baddbmm..  矩阵运算\n",
    "t\t                               转置\n",
    "dot/cross\t                       内积/外积\n",
    "inverse\t                           求逆矩阵\n",
    "svd\t                               奇异值分解\n",
    "”””"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2]])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[2, 3],\n",
      "        [3, 4],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "# 广播机制\n",
    "import torch\n",
    "x = torch.arange(1, 3).view(1, 2)\n",
    "print(x)\n",
    "y = torch.arange(1, 4).view(3, 1)\n",
    "print(y)\n",
    "print(x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.] tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
      "[2. 2. 2. 2. 2.] tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
      "[3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n",
      "[4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# #tensor 转 numpy\n",
    "# a = torch.ones(5)\n",
    "# b = a.numpy()\n",
    "# print(a,b)\n",
    "# a += 1\n",
    "# print(a,b)\n",
    "# b += 1\n",
    "# print(a,b)\n",
    "\n",
    "#numpy 转 tensor\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "print(a,b)\n",
    "a += 1\n",
    "print(a,b)\n",
    "b += 1\n",
    "print(a,b)\n",
    "\n",
    "c = torch.tensor(a)\n",
    "print(c)\n",
    "a += 1\n",
    "print(a,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "None\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f09f7c5a198>\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n",
      "True False\n",
      "a: False\n",
      "new_a: True\n",
      "<SumBackward0 object at 0x7f09f76f54e0>\n",
      "out关于x的梯度： tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n",
      "out2: tensor(4., grad_fn=<SumBackward0>)\n",
      "out2_grad: tensor([[5.5000, 5.5000],\n",
      "        [5.5000, 5.5000]])\n",
      "out3: tensor(4., grad_fn=<SumBackward0>)\n",
      "out3_grad: tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#grad_fn 会返回通过运算得出的tensor的运算,初始tensor的grad_fn默认None\n",
    "#requires_grad=True 相当于将tensor设计为leaf\n",
    "import torch\n",
    "x = torch.ones(2,2,requires_grad=True)\n",
    "print(x)\n",
    "print(x.grad_fn) #x为input值，None  leaf节点\n",
    "\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn)\n",
    "\n",
    "z = y*y*3\n",
    "out = z.mean()\n",
    "print(z,out)\n",
    "print(x.is_leaf,y.is_leaf) #判定x y 是否为leaf节点\n",
    "\n",
    "#requires_grad_() 来改变r_g属性默认false\n",
    "a = torch.randn(2,2)\n",
    "a = ((a*3)/(a-1))\n",
    "print('a:',a.requires_grad) #False\n",
    "a.requires_grad_()  #默认改变，True\n",
    "print('new_a:',a.requires_grad) #True\n",
    "b = (a*a).sum()\n",
    "print(b.grad_fn)\n",
    "\n",
    "#梯度\n",
    "out.backward()\n",
    "print('out关于x的梯度：',x.grad)\n",
    "\n",
    "out2 = x.sum()\n",
    "out2.backward()\n",
    "print('out2:',out2)\n",
    "print('out2_grad:',x.grad)\n",
    "\n",
    "out3 = x.sum()\n",
    "print('out3:',out3)\n",
    "x.grad.data.zero_()  #消除累计梯度\n",
    "out3.backward()\n",
    "print('out3_grad:',x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]], grad_fn=<ViewBackward>)\n",
      "tensor([2.0000, 0.2000, 0.0200, 0.0020])\n",
      "True\n",
      "tensor(1., grad_fn=<PowBackward0>) True\n",
      "tensor(1.) False\n",
      "tensor(2., grad_fn=<AddBackward0>) True\n",
      "tensor(2.)\n"
     ]
    }
   ],
   "source": [
    "#注意在y.backward()时，如果y是标量，则不需要为backward()传入任何参数；\n",
    "#否则，需要传入一个与y同形的Tensor\n",
    "\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
    "y = 2 * x\n",
    "z = y.view(2, 2)\n",
    "print(z)  #现在 z 不是一个标量，所以在调用backward时需要传入一个\n",
    "                #和z同形的权重向量进行加权求和得到一个标量。\n",
    "\n",
    "v = torch.tensor([[1.0, 0.1], [0.01, 0.001]], dtype=torch.float)\n",
    "z.backward(v)\n",
    "print(x.grad)  #注:x.grad和x是同型张量\n",
    "                #z对x的梯度\n",
    "    \n",
    "# torch.no_grad()  来中断梯度追踪\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y1 = x ** 2 \n",
    "with torch.no_grad():\n",
    "    y2 = x ** 3\n",
    "y3 = y1 + y2\n",
    "\n",
    "print(x.requires_grad)\n",
    "print(y1, y1.requires_grad) # True\n",
    "print(y2, y2.requires_grad) # False\n",
    "print(y3, y3.requires_grad) # True\n",
    "\n",
    "y3.backward()\n",
    "print(x.grad)  #结果是‘2’，因为y2被隐藏排除梯度计算中，y3对x求梯度只计算了y2部分\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " tensor([1.], requires_grad=True)\n",
      "x.data:\n",
      " tensor([1.])\n",
      "x.data的r_g值：\n",
      " False\n",
      "y:\n",
      " tensor([2.], grad_fn=<MulBackward0>)\n",
      "修改x数据后的x：\n",
      " tensor([100.], requires_grad=True)\n",
      "y对new_x的导数：\n",
      " tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "#利用 tensor.data 来修改tensor值，并且不会被autograd记录从而影响反向传播\n",
    "\n",
    "import torch\n",
    "x = torch.ones(1,requires_grad=True)\n",
    "print('x:\\n',x)\n",
    "print('x.data:\\n',x.data) # 还是一个tensor\n",
    "print('x.data的r_g值：\\n',x.data.requires_grad) # 但是已经是独立于计算图之外\n",
    "\n",
    "y = 2 * x\n",
    "x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播\n",
    "print('y:\\n',y)\n",
    "y.backward()\n",
    "print('修改x数据后的x：\\n',x) # 更改data的值也会影响tensor的值\n",
    "print('y对new_x的导数：\\n',x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
